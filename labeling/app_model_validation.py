
'''
This little streamlit app is used to evaluate the performance of different models for
labeling sustainability reports for the project businessresponsibility.ch

For more information visit https://github.com/bizres
'''

import streamlit as st
import pandas as pd

import data_importer # Load tool kit to import training and test data
import validator # Load tool kti to validate predictions

from predictor import Predictor # Import a predictor class

st.title('Businessresponsibility.ch')
st.markdown('**Sustainability Report Content Classification Model Performance**')

# Declare labels for which the model should be trained. The labels need to match the labels in the training and test data
labels = ['human_rights', 'environment', 'corruption', 'social_concerns', 'employee_concerns'] 

st.header('Choose Model Parameters')
# The values of the select boxes are used during the instantiation of the predictor class.
label_choice = st.selectbox('Select the Label you want your model to detect:', labels)
vectorization_choice = st.selectbox('Select the mode of text vectorization:', ['bag_of_words', 'spacy_document_vectors'])
prediction_choice = st.selectbox('Select prediction algorithm:', ['logistic_regression', 'naive_bayes'])
optimization_choice = st.selectbox('Select metric to optimize:', ['none', 'accuracy', 'recall'])

st.header('Choose Validation Parameters')
# This parameter sets the number of rounds which are used to validate the model. E.g. 5 means that the validation results are the mean of the performance during 5 runs.
run_choice = st.selectbox('Select how often the model should be validated (more runs means a more reliable average):', [1, 5, 10, 20])
split_choice = st.selectbox('Select the ratio of training and testing data:', [0.8, 0.9, 0.7])

# All those variables are used to summarise the results of the multiple validation runs
accuracies = []
recalls = []
false_neg_rates = []
false_pos_rates = []
false_negs = []
false_poss = []
df_train_sizes = []
nums_of_train_labels = []
df_test_sizes = []
nums_of_test_labels = []

st.header('Validate Model')

# This button prevents the validation from running whenever one of the select boxes is changed
if st.button('Run'):
    st.markdown('Validating Model in {} runs...'.format(run_choice))
    progress_bar = st.progress(0)

    for run in range(run_choice):

        with st.spinner('Initializing Model...'):
            # Create a new instance of the predictor class
            model = Predictor(labels, vectorization_choice, prediction_choice, optimization_choice)

            # load the training and testing data for the run
            train_df, test_df = data_importer.load_train_and_validation_data(split_choice)

        with st.spinner('Training and testing model for run no. {}...'.format(run+1)):

            # Train model and generate predictions
            model.train(train_df['text'], train_df['label'])
            predictions = model.predict(test_df['text'], label_choice)

            # the val variable contains the results of one validation run
            val = validator.calculate_statistics(test_df['label'], predictions, label_choice, test_df['text'])
            
            # if multiple validations are running, we need to keep track of the individual results
            df_train_sizes.append(len(train_df))
            df_test_sizes.append(len(test_df))
            nums_of_train_labels.append(train_df['label'].tolist().count(label_choice))
            nums_of_test_labels.append(test_df['label'].tolist().count(label_choice))

            accuracies.append((val['true_pos'] + val['true_neg'])/(val['pos'] + val['neg']))
            recalls.append(val['true_pos']/val['pos'])
            false_neg_rates.append(val['false_neg']/val['pos'])
            false_pos_rates.append(val['false_pos']/val['neg'])

            # For the last run we want to show false negatives and false positives
            false_negs = pd.DataFrame(val['false_neg_entries'], columns=['False Negatives'])
            false_poss = pd.DataFrame(val['false_pos_entries'], columns=['False Positives'])

            progress_bar.progress(run/run_choice + ((1/run_choice)))

    # Show results
    st.markdown('The size of the **training data was {} texts** which contained **in average texts {} labeled as {}**'.format(int(sum(df_train_sizes)/len(df_train_sizes) + 0.5), int(sum(nums_of_train_labels)/len(nums_of_train_labels)+0.5), label_choice))
    st.markdown('The size of the **test data was {} texts** which contained **in average texts {} labeled as {}**'.format(int(sum(df_test_sizes)/len(df_test_sizes) + 0.5), int(sum(nums_of_test_labels)/len(nums_of_test_labels)+0.5), label_choice))

    st.subheader('Average Accuracy: {:.2%}'.format(sum(accuracies)/len(accuracies)))
    st.info('The accuracy depicts what share of predictions generated by the model are correct.')
    st.subheader('Average Recall Rate: {:.2%}'.format(sum(recalls)/len(recalls)))
    st.info('The recall depicts what share of the texts labeled as {} are correctly labeled as such by the model.'.format(label_choice))
    st.subheader('Average False Negative Rate: {:.2%}'.format(sum(false_neg_rates)/len(false_neg_rates)))
    st.info('The false negative rate depicts what share of texts labeled as {} are wrongfully **not** labeled as such by the model.'.format(label_choice))
    st.subheader('Average False Positive Rate: {:.2%}'.format(sum(false_pos_rates)/len(false_pos_rates)))
    st.info('The false positive rate depicts what share of texts **not** labeled as {} are wrongfully labeled as such by the model.'.format(label_choice))

    st.markdown('**In the last run the following texts were wrongfully classified as {}**'.format(label_choice))
    st.table(false_poss)

    st.markdown('**In the last run the following texts were wrongfully **not** classified as {}**'.format(label_choice))
    st.table(false_negs)